{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install autogen-agentchat\n",
    "!pip install autogen-agentchat~=0.2\n",
    "!pip install ratelimit\n",
    "!pip install tiktoken\n",
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "from autogen import ConversableAgent\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"place_holder\" # Replace with your actual API key\n",
    "\n",
    "# QPS and concurrency limits\n",
    "MAX_QPS = 2  # 每秒最多请求数\n",
    "MAX_CONCURRENT_REQUESTS = 2  # 最大并发请求数\n",
    "\n",
    "# Token limit for different models\n",
    "DEFAULT_TOKEN_LIMIT = 8192  # 默认 Token 限制（8k）\n",
    "LLAMA_TOKEN_LIMIT = 20480   # Llama 模型 Token 限制（80k）\n",
    "OPENAI_TOKEN_LIMIT = 20480 # OpenAI GPT-4 Turbo 模型 Token 限制（100k）\n",
    "\n",
    "\n",
    "# Load data from file\n",
    "TRAIN_FILE_100 = 'data/v1.0-simplified_nq-dev-all_sample100_seed42.jsonl'\n",
    "DEV_FILE_100 = 'data/v1.0-simplified_nq-dev-all_sample100_seed42.jsonl'\n",
    "SINGLE_ENTRY_FILE= 'data/first_entry_sample.jsonl'\n",
    "\n",
    "# Configure LLM provider\n",
    "LLM_PROVIDER = \"ollama\"\n",
    "OLLAMA_API_BASE = \"http://localhost:11434/v1\"\n",
    "MAX_TOKENS = LLAMA_TOKEN_LIMIT if LLM_PROVIDER == \"ollama\" else OPENAI_TOKEN_LIMIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_config():\n",
    "    llm_configs = {\n",
    "        \"ollama\": {\n",
    "            \"config_list\": [\n",
    "                {\n",
    "                    \"model\": \"llama3.2:latest\",\n",
    "                    \"api_key\": \"ollama\",\n",
    "                    \"base_url\": OLLAMA_API_BASE,\n",
    "                    \"temperature\": 0.7,\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"openai\": {\n",
    "            \"config_list\": [\n",
    "                {\n",
    "                    \"model\": \"gpt-4o\",\n",
    "                    \"api_key\": os.environ.get(\"OPENAI_API_KEY\"),\n",
    "                    \"temperature\": 0.7,\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # check llm config\n",
    "    if LLM_PROVIDER not in llm_configs:\n",
    "        raise ValueError(f\"Unsupported LLM provider: {LLM_PROVIDER}\")\n",
    "\n",
    "    # return llm config\n",
    "    return llm_configs[LLM_PROVIDER]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "# Define functions to read and parse JSONL files\n",
    "def read_first_lines(file_path, num_lines=5):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for _ in range(num_lines):\n",
    "            print(file.readline().strip())\n",
    "\n",
    "def parse_jsonl(file_path, num_lines=None):\n",
    "    output = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for idx, line in enumerate(file):\n",
    "            if num_lines is not None and idx >= num_lines:\n",
    "                break\n",
    "            data = json.loads(line.strip())\n",
    "            output.append(data)\n",
    "    return output\n",
    "\n",
    "def list_to_jsonl(data_list, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for item in data_list:\n",
    "            json_line = json.dumps(item, ensure_ascii=False)\n",
    "            file.write(json_line + '\\n')\n",
    "\n",
    "# Define helper functions for processing NQ data\n",
    "def get_nq_tokens(simplified_nq_example):\n",
    "    \"\"\"Returns list of blank-separated tokens.\"\"\"\n",
    "    if \"document_text\" not in simplified_nq_example:\n",
    "        raise ValueError(\"`get_nq_tokens` should be called on a simplified NQ example that contains the `document_text` field.\")\n",
    "    return simplified_nq_example[\"document_text\"].split(\" \")\n",
    "\n",
    "def get_short_answers(nq_example):\n",
    "    document_tokens = get_nq_tokens(nq_example)\n",
    "    short_answers = []\n",
    "    for annotation in nq_example['annotations']:\n",
    "        if annotation['short_answers']:\n",
    "            for short_answer in annotation['short_answers']:\n",
    "                short_answer_text = ' '.join(\n",
    "                    document_tokens[short_answer['start_token']:short_answer['end_token']]\n",
    "                )\n",
    "                short_answers.append(short_answer_text)\n",
    "    return short_answers\n",
    "\n",
    "def strip_end_punctuation(text):\n",
    "    punctuation = '.!?,;:)\"'\n",
    "    text = text.strip()\n",
    "    while text and text[-1] in punctuation:\n",
    "        text = text[:-1].strip()\n",
    "    return text\n",
    "\n",
    "json_top100 = parse_jsonl(TRAIN_FILE_100, num_lines=100)\n",
    "json_top100_dev = parse_jsonl(DEV_FILE_100, num_lines=100)\n",
    "json_single_entry = parse_jsonl(SINGLE_ENTRY_FILE, num_lines=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_single_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_Data_List = json_single_entry\n",
    "\n",
    "# Implement RAG by defining a retrieval function\n",
    "# def retrieve_relevant_data_with_answers(question, data_list, top_k=5):\n",
    "#     # For simplicity, retrieve entries where the question text contains keywords from the input question\n",
    "#     question_keywords = set(question.lower().split())\n",
    "#     relevance_scores = []\n",
    "#     for item in data_list:\n",
    "#         item_keywords = set(item['question_text'].lower().split())\n",
    "#         common_keywords = question_keywords.intersection(item_keywords)\n",
    "#         score = len(common_keywords)\n",
    "#         relevance_scores.append((score, item))\n",
    "#     # Sort based on relevance score\n",
    "#     relevance_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "#     # Get top_k relevant data\n",
    "#     retrieved_data = [item for score, item in relevance_scores if score > 0][:top_k]\n",
    "#     return retrieved_data\n",
    "\n",
    "def process_data(data_list):\n",
    "    # For simplicity, retrieve entries where the question text contains keywords from the input question\n",
    "    processed_document_list = []\n",
    "    for item in data_list:\n",
    "        question_text = item['question_text']\n",
    "        document_text = item['document_text']\n",
    "        example_id = item['example_id']\n",
    "        item_keywords = set(item['question_text'].lower().split())\n",
    "        processed_item = {\n",
    "            'question_text': question_text,\n",
    "            'document_text': document_text,\n",
    "            'example_id': example_id,\n",
    "            'item_keywords': item_keywords\n",
    "        }\n",
    "        processed_document_list.append(processed_item)\n",
    "    # Sort based on relevance score\n",
    "    # relevance_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    # Get top_k relevant data\n",
    "    # retrieved_data = [item for score, item in relevance_scores if score > 0][:top_k]\n",
    "    return processed_document_list\n",
    "\n",
    "\n",
    "processed_data = process_data(target_Data_List)\n",
    "# Example usage of the retrieval function\n",
    "single_data = processed_data[0]\n",
    "single_question = processed_data[0]['question_text']\n",
    "single_retrieved_document = processed_data[0]['document_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ExtractContentAgent\n",
    "def create_extract_content_agent():\n",
    "    return ConversableAgent(\n",
    "        name=\"ExtractContentAgent\",\n",
    "        system_message=\"\"\"\n",
    "#Role:\n",
    "You are an expert text extractor that identifies exact passages from a given context that directly answer or relate to a specific question.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\t1.\t**Extract Exact Text**: Provide the **exact subsequences** from the context that may **contain or directly answer the question**. The extracted text must be **word-for-word from the original context** without any changes.\n",
    "\t2.\t**No Alterations**: Do not paraphrase, summarize, or add any additional information. Avoid introducing personal opinions or external knowledge.\n",
    "\t3.\t**Multiple Passages**: If multiple parts of the context are relevant, include all relevant excerpts.\n",
    "\t4.\t**Step-by-Step Analysis**: Carefully analyze the context step-by-step to identify all passages that are relevant to the question.\n",
    "\t5.\t**Formatting**: Your output must follow the following format.\n",
    "# Format\n",
    "## Input Format\n",
    "\n",
    "[Question]  Question Text\n",
    "[Context] Document Text\n",
    "\n",
    "## Output Format\n",
    "{\n",
    "    \"Relevant_Context\": \"Relevant Context Place Holder\"\n",
    "}\n",
    "\n",
    "# Examples\n",
    "## Example 1\n",
    "**Input:**\n",
    "[Question] What is the capital city of Japan?\n",
    "[Context] Tokyo is the capital city of Japan. It is one of the largest cities in the world.\n",
    "\n",
    "**Output:**\n",
    "{\n",
    "    \"Relevant_Context\": \"Tokyo is the capital city of Japan.\"\n",
    "}\n",
    "\n",
    "## Example 2\n",
    "**Input:**\n",
    "[Question] Which element has the atomic number 6?\n",
    "[Context] Carbon has the atomic number 6 and is essential to all known life forms.\n",
    "\n",
    "**Output:**\n",
    "{\n",
    "    \"Relevant_Context\": \"Carbon has the atomic number 6 and is essential to all known life forms.\"\n",
    "}\n",
    "### Example 3\n",
    "\n",
    "**Input:**\n",
    "[Question] Who painted the Mona Lisa?\n",
    "[Context] Leonardo da Vinci was a Renaissance artist known for masterpieces such as the Mona Lisa and The Last Supper.\n",
    "\n",
    "**Output:**\n",
    "{\n",
    "    \"Relevant_Context\": \"Leonardo da Vinci was a Renaissance artist known for masterpieces such as the Mona Lisa and The Last Supper.\"\n",
    "}\n",
    "\"\"\",\n",
    "        llm_config=get_llm_config(),\n",
    "        human_input_mode=\"NEVER\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Create JudgerAgent\n",
    "def create_judger_agent():\n",
    "    return ConversableAgent(\n",
    "        name=\"JudgerAgent\",\n",
    "        system_message=\"\"\"You are a judging agent who evaluates the relevance of extracted passages to the given question. \n",
    "Your task is to:\n",
    "1. Assess each extracted passage and rate its relevance to the question.\n",
    "2. Choose the top 3 most relevant passages.\n",
    "3. If only one passage is clearly the best match, return that one passage.\n",
    "\n",
    "Output Format:\n",
    "- [Top_Passages] (or the single best passage if applicable).\n",
    "{\n",
    "    \"Top_Passages\": Top_Passages place holder\n",
    "}\n",
    "\"\"\",\n",
    "        llm_config=get_llm_config(),\n",
    "        human_input_mode=\"NEVER\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(agent, chunk, question):\n",
    "    input_message = f\"[Question] {question}\\n[Context] {chunk}\"\n",
    "    response = agent.generate_reply(messages=[{\"content\": input_message, \"role\": \"user\"}])\n",
    "    #return response.get(\"Relevant_Context\", \"\")\n",
    "    return response\n",
    "\n",
    "def judge_relevance(agent, extracted_contents, question):\n",
    "    input_message = f\"Evaluate the relevance of the following passages to the question: {question}\\nPassages: {extracted_contents}\"\n",
    "    print(\"input_message: \", input_message)\n",
    "    response = agent.generate_reply(messages=[{\"content\": input_message, \"role\": \"user\"}])\n",
    "    #return response.get(\"Top_Passages\", \"\")\n",
    "    return response\n",
    "\n",
    "# def split_document(document, chunk_size):\n",
    "#     tokens = document.split()\n",
    "#     return [' '.join(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "# split documents into chunks by token limit\n",
    "def split_document(document, model_name=\"gpt-4\", max_tokens=DEFAULT_TOKEN_LIMIT):\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "    tokens = tokenizer.encode(document)\n",
    "\n",
    "    # split documents into chunks by token limit\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "        start = end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=MAX_QPS, period=1)\n",
    "def extract_content_with_rate_limit(agent, chunk, question):\n",
    "    return extract_content(agent, chunk, question)\n",
    "\n",
    "# 获取 OpenAI 的 Tokenizer\n",
    "def get_tokenizer(model_name=\"gpt-4\"):\n",
    "    try:\n",
    "        return tiktoken.encoding_for_model(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer for model {model_name}: {e}\")\n",
    "        # 默认使用 GPT-3.5 的 Tokenizer\n",
    "        return tiktoken.get_encoding(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(single_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_document_discussion(data, max_turns: int = 3, model_name: str = \"gpt-4o\", max_tokens: int = DEFAULT_TOKEN_LIMIT):\n",
    "    document = data[\"document_text\"]\n",
    "    question = data[\"question_text\"]\n",
    "    # Step 1\n",
    "    chunks = split_document(document, model_name=model_name, max_tokens=max_tokens)\n",
    "    # Step 2\n",
    "    extract_agents = [create_extract_content_agent() for _ in range(len(chunks))]\n",
    "\n",
    "    extracted_contents = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:\n",
    "        future_to_chunk = {\n",
    "            executor.submit(extract_content_with_rate_limit, agent, chunk, question): chunk\n",
    "            for agent, chunk in zip(extract_agents, chunks)\n",
    "        }\n",
    "\n",
    "        for future in future_to_chunk:\n",
    "            try:\n",
    "                result = future.result()\n",
    "                # print(result)\n",
    "                extracted_contents.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during extraction: {e}\")\n",
    "\n",
    "    # Step 3: Use JudgerAgent to judge reference\n",
    "    judger_agent = create_judger_agent()\n",
    "    # print(\"!!!!\" + \"extracted_contents\", extracted_contents)\n",
    "    top_passages = judge_relevance(judger_agent, extracted_contents, question)\n",
    "\n",
    "    # Step 4: output answer\n",
    "    if len(top_passages) == 1:\n",
    "        print(\"Best Matched Passage:\", top_passages[0])\n",
    "    else:\n",
    "        print(\"Top 3 Relevant Passages:\" + top_passages)\n",
    "        for passage in top_passages:\n",
    "            print(\"\\n\\n\\n-\", passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = single_document_discussion(single_data, max_turns=3, model_name=\"gpt-4o\", max_tokens=OPENAI_TOKEN_LIMIT)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
